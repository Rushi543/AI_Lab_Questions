import numpy as np


# Define the MDP parameters
probability = [0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]
rewards = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000]

def monte_carlo(episodes):
    totalReward=0
    for i in range(episodes):
        state = 0
        reward = 0
        policy = [1]*10
        
        while True:
            if state>9:
                break
            action = policy[state]
            if action==0:
                break
            chance = np.random.rand()
            
            if chance<probability[state]:
                reward+=rewards[state]
                state+=1
            else:
                break    
        totalReward+=reward
    print(i)
    return totalReward



# Run the Monte Carlo simulation with the initial policy
num_episodes = 10000000
avg_reward = monte_carlo(num_episodes)/num_episodes

# Print the expected average return
print("Expected average return:", avg_reward)    
